# -*- coding: utf-8 -*-
"""Customer_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/NikunjM1703/Consumer-Personality-Analysis/blob/main/Customer_Segmentation.ipynb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from datetime import date
import warnings

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, silhouette_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from imblearn.over_sampling import SMOTE # Import SMOTE for handling imbalance

# Set styles and suppress
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("viridis")

"""Loading Dataset and checking"""

df = pd.read_excel('marketing_campaign.xlsx')

df.head()

df.info()

df.isnull().sum()

"""EDA and feature engineering
1. filled missing income entries with the median salary
2. marital status categorization
3. addition of age, children count, family size, spend, channel purchased, campaign accepted and tenure columns
4. checked for outliers in age, income, tenure and recency
5. removed outliers in age and income
6. checked for duplicate ids
"""

# EDA
income_median = df['Income'].median()
df.fillna({'Income':df['Income'].median()}, inplace=True)
print(f"Missing 'Income' values filled with median: {income_median}")

df['Marital_Status'] = df['Marital_Status'].replace({
    'Married': 'Partner',
    'Together': 'Partner',
    'Divorced': 'Single',
    'Widow': 'Single',
    'Alone': 'Single',
    'Absurd': 'Single',
    'YOLO': 'Single'})

df.head()

# Removing outliers in age and income
current_year = datetime.now().year
df['Age'] = current_year - df['Year_Birth']

plt.figure(figsize=(10,6))
sns.scatterplot(x='Age', y='Income', data=df)

plt.title('Income vs Age (Outliers visible)')
plt.xlabel('Age')
plt.ylabel('Income')

plt.grid(True)
plt.show()

# Remove outliers in Age and Income using IQR method

# Compute IQR for Age
Q1_age = df['Age'].quantile(0.25)
Q3_age = df['Age'].quantile(0.75)
IQR_age = Q3_age - Q1_age
lower_age = Q1_age - 1.5 * IQR_age
upper_age = Q3_age + 1.5 * IQR_age

# Compute IQR for Income
Q1_inc = df['Income'].quantile(0.25)
Q3_inc = df['Income'].quantile(0.75)
IQR_inc = Q3_inc - Q1_inc
lower_inc = Q1_inc - 1.5 * IQR_inc
upper_inc = Q3_inc + 1.5 * IQR_inc

# Filter out outliers
df_clean= df[(df['Age'] >= lower_age) & (df['Age'] <= upper_age) &
              (df['Income'] >= lower_inc) & (df['Income'] <= upper_inc)]

print(f"Cleaned shape: {df_clean.shape}")
print(f"Removed {df.shape[0] - df_clean.shape[0]} outliers.")

df= df_clean

# plot after removing outliers in age and income
current_year = datetime.now().year
df['Age'] = current_year - df['Year_Birth']

plt.figure(figsize=(10,6))
sns.scatterplot(x='Age', y='Income', data=df)

plt.title('Income vs Age (Outliers visible)')
plt.xlabel('Age')
plt.ylabel('Income')

plt.grid(True)
plt.show()

# checking for outliers in recency
plt.figure(figsize=(10,6))
sns.scatterplot(x='Age', y='Recency', data=df)

plt.title('Recency vs Age (Outliers visible)')
plt.xlabel('Age')
plt.ylabel('Recency')

plt.grid(True)
plt.show()

# Convert Dt_Customer to a datetime and create a tenure feature
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'])
df['Days_As_Customer'] = (datetime.now() - df['Dt_Customer']).dt.days

#checking for outliers in tenure
plt.figure(figsize=(10,6))
sns.boxplot(x='Age', y='Days_As_Customer', data=df)

plt.title('Days_As_Customer vs Age (Outliers visible)')
plt.xlabel('Age')
plt.ylabel('Days_As_Customer')

plt.grid(True)
plt.show()

# Check for duplicate IDs
duplicate_ids = df[df.duplicated(subset='ID', keep=False)]
print("Duplicate ID entries:\n", duplicate_ids)

# Number of duplicate rows
print("\nNumber of duplicate rows:", duplicate_ids.shape[0])

# Drop duplicate IDs (keep the first occurrence)
df = df.drop_duplicates(subset='ID', keep='first')

# Verify removal
print("\nShape after dropping duplicates:", df.shape)
print("Any duplicates left?", df.duplicated(subset='ID').any())

# household structure
df['ChildrenCount'] = df['Kidhome'] + df['Teenhome']
df['Family_Size'] = df['ChildrenCount'] + df['Marital_Status'].apply(lambda x: 2 if x == 'Partner' else 1)
df['marital_status'] = df['Marital_Status'].apply(lambda x: 1 if x == 'Partner' else 0)

# Create aggregate spending, purchase channel, and campaign acceptance features
spending_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
df['Total_Spent'] = df[spending_cols].sum(axis=1)

purchase_cols = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']
df['Total_Purchases'] = df[purchase_cols].sum(axis=1)

campaign_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']
df['Total_Campaign_Accepted'] = df[campaign_cols].sum(axis=1)

"""DATA PREPROCESSING"""

#ORDINALLY ENCODING EDUCATION
Education_Encoded = {'Basic': 1, '2n Cycle': 2, 'Graduation': 3, 'Master': 4,'PhD': 5}
df['Education_Encoded'] = df['Education'].map(Education_Encoded)

columns_to_drop = ['ID','Year_Birth','Education','Marital_Status','Kidhome','Teenhome','Dt_Customer', 'Z_CostContact',
                   'Z_Revenue','Complain','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']

# Drop columns that exist in the DataFrame
existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df = df.drop(existing_columns_to_drop, axis=1)

# Optionally, you might want to verify which columns were dropped
print(f"Dropped columns: {existing_columns_to_drop}")

df.head()

#analysis
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
# Age distribution
sns.histplot(data=df, x='Age', bins=30, alpha=0.7, ax=axes[0,0], kde=True)
axes[0,0].set_title('Age Distribution')
axes[0,0].set_xlabel('Age')

# Income distribution
sns.histplot(data=df, x='Income', bins=30, alpha=0.7, ax=axes[0,1], kde=True)
axes[0,1].set_title('Income Distribution')
axes[0,1].set_xlabel('Income')

# Total spending distribution
sns.histplot(data=df, x='Total_Spent', bins=30, alpha=0.7, ax=axes[0,2], kde=True)
axes[0,2].set_title('Total Spending Distribution')
axes[0,2].set_xlabel('Total Spent')

# education distribution
axes[1,0].hist(df['Education_Encoded'], bins=30, alpha=0.7)
axes[1,0].set_title('Education Distribution')
axes[1,0].set_xlabel('Education')

# Spending by product category
spending_by_category = df[spending_cols].mean()
axes[1,1].bar(range(len(spending_cols)), spending_by_category.values)
axes[1,1].set_title('Average Spending by Product Category')
axes[1,1].set_xticks(range(len(spending_cols)))
axes[1,1].set_xticklabels([col.replace('Mnt', '') for col in spending_cols], rotation=45)

# Purchase channels
purchase_by_channel = df[purchase_cols].mean()
axes[1,2].bar(range(len(purchase_cols)), purchase_by_channel.values)
axes[1,2].set_title('Average Purchases by Channel')
axes[1,2].set_xticks(range(len(purchase_cols)))
axes[1,2].set_xticklabels([col.replace('Num', '').replace('Purchases', '') for col in purchase_cols], rotation=45)

plt.tight_layout()
plt.show()

# Calculate skewness for each column
skewness = df.skew()

print("Skewness of each column:")
print(skewness)

# Apply log transformation to columns with absolute skewness >= 0.5
skewed_cols = skewness[abs(skewness) >= 0.5].index

for col in skewed_cols:
    # Apply log1p to handle potential zero values
    df[col] = np.log1p(df[col])

print("\nColumns with absolute skewness >= 0.5 transformed using log1p:")
print(skewed_cols)

# Verify skewness after transformation
print("\nSkewness after transformation:")
print(df.skew())

# (a) Boxplots for spending vs Nnmber of accepted campaigns
for col in spending_cols + ['Total_Spent']:
    plt.figure(figsize=(7,4))
    sns.boxplot(x='Total_Campaign_Accepted', y=col, data=df)
    plt.title(f'{col} vs Campaign Response')
    plt.xlabel('Total_Campaign_Accepted (1=Accepted)')
    plt.ylabel(col)
    plt.show()

# (b) Boxplots for purchase channels vs response
for col in purchase_cols:
    plt.figure(figsize=(7,4))
    sns.boxplot(x='Total_Campaign_Accepted', y=col, data=df)
    plt.title(f'{col} vs Campaign Response')
    plt.xlabel('Total_Campaign_Accepted (1=Accepted)')
    plt.ylabel(col)
    plt.show()

# (c) Scatterplot for interaction between web & catalog purchases
plt.figure(figsize=(7,5))
sns.scatterplot(x='NumWebPurchases', y='NumCatalogPurchases',
                hue='Total_Campaign_Accepted', data=df, alpha=0.6)
plt.title('Web vs Catalog Purchases by Campaign Response')
plt.xlabel('Web Purchases')
plt.ylabel('Catalog Purchases')
plt.legend(title='Total_Campaign_Accepted')
plt.show()

# (d) Recency vs response
plt.figure(figsize=(7,4))
sns.histplot(data=df, x='Recency', hue='Total_Campaign_Accepted', bins=30, kde=True, multiple='stack')
plt.title('Recency vs Campaign Response')
plt.xlabel('Recency (days since last purchase)')
plt.ylabel('Number of Customers')
plt.show()

# (e) Demographics vs response
plt.figure(figsize=(7,4))
sns.boxplot(x='Total_Campaign_Accepted', y='Income', data=df)
plt.title('Income vs Campaign Response')
plt.show()

plt.figure(figsize=(7,4))
sns.boxplot(x='Total_Campaign_Accepted', y='Age', data=df)
plt.title('Age vs Campaign Response')
plt.show()

# Correlation heatmap
plt.figure(figsize=(15, 10)) # Increased figure size for better readability
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0, fmt=".2f") # Include all features and format annotations
plt.title('All Feature Correlation Matrix')
plt.show()

"""K-Means Clustering"""

##customer segmentation
# Select features for clustering analysis
clustering_features = df.columns.tolist()


X_clustering = df[clustering_features].copy()


# Standardise features as K-Means is distance-based
scaler_clustering = StandardScaler()
X_clustering_scaled = scaler_clustering.fit_transform(X_clustering)

inertias = []
silhouette_scores = []
k_range_inertia = range(1, 11) # Start k from 1 for inertia
k_range_silhouette = range(2, 11) # Start k from 2 for silhouette score

for k in k_range_inertia:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_clustering_scaled)
    inertias.append(kmeans.inertia_)

for k in k_range_silhouette:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_clustering_scaled)
    silhouette_scores.append(silhouette_score(X_clustering_scaled, kmeans.labels_))


# Plot Elbow Curve and Silhouette Scores to determine k
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
ax1.plot(k_range_inertia, inertias, 'bo-')
ax1.set_xlabel('Number of Clusters (k)')
ax1.set_ylabel('Inertia')
ax1.set_title('Elbow Method for Optimal k')

ax2.plot(k_range_silhouette, silhouette_scores, 'ro-')
ax2.set_xlabel('Number of Clusters (k)')
ax2.set_ylabel('Silhouette Score')
ax2.set_title('Silhouette Score vs. Number of Clusters')
plt.tight_layout()
plt.show()

# Apply K-Means with optimal number of clusters
optimal_k = 4
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans_final.fit_predict(X_clustering_scaled)


print(f"Clustering completed with {optimal_k} clusters")
print(f"Cluster distribution:")
print(df['Cluster'].value_counts().sort_index())

# --- Cluster Profiling ---
profile_features_for_analysis = ['Age', 'Income', 'Total_Spent', 'Total_Purchases', 'Family_Size']
cluster_profiles = df.groupby('Cluster')[profile_features_for_analysis].mean().round(2)
print("\n--- Cluster Profiles ---")
print(cluster_profiles)
print("\n")

# Use PCA to visualize the clusters in 3D
pca_3d = PCA(n_components=3)
X_pca_3d = pca_3d.fit_transform(X_clustering_scaled)

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Use df['Cluster'] for coloring
scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],
                     c=df['Cluster'], cmap='viridis', s=50, alpha=0.7)

ax.set_title('Customer Segments Visualized with 3D PCA')
ax.set_xlabel(f'Principal Component 1 ({pca_3d.explained_variance_ratio_[0]:.1%})')
ax.set_ylabel(f'Principal Component 2 ({pca_3d.explained_variance_ratio_[1]:.1%})')
ax.set_zlabel(f'Principal Component 3 ({pca_3d.explained_variance_ratio_[2]:.1%})')

# Add a legend
legend = ax.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

# Apply PCA with 2 components
pca_2d = PCA(n_components=2)
X_pca_2d = pca_2d.fit_transform(X_clustering_scaled)

# Plot the 2D PCA results
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1],
                      c=df['Cluster'], cmap='viridis', s=50, alpha=0.7)

plt.title('Customer Segments Visualized with 2D PCA')
plt.xlabel(f'Principal Component 1 ({pca_2d.explained_variance_ratio_[0]:.1%})')
plt.ylabel(f'Principal Component 2 ({pca_2d.explained_variance_ratio_[1]:.1%})')

# Add a legend
legend = plt.legend(*scatter.legend_elements(), title="Cluster")
plt.show()

# Interpret Cluster Profiles
print("\n--- Interpretation of Cluster Profiles ---")
print("Cluster 0: Moderate Age, Moderate Income, Moderate Spending & Purchases, Larger Family Size")
print("   - Characteristics: Average age and income, moderate spending and purchase frequency, tend to have larger families.")
print("Cluster 1: Younger/Middle Age, Lower Income, Low Spending & Purchases, Average Family Size")
print("   - Characteristics: Younger or middle-aged, significantly lower income, very low spending and purchase frequency, average family size.")
print("Cluster 2: Older/Middle Age, Higher Income, High Spending & Purchases, Smaller Family Size")
print("   - Characteristics: Older or middle-aged, higher income, high spending and purchase frequency, tend to have smaller families (often without children at home).")
print("Cluster 3: Middle Age, Highest Income, Highest Spending & Purchases, Smaller Family Size")
print("   - Characteristics: Middle-aged, highest income, highest spending and purchase frequency, tend to have smaller families (often without children at home).")


# Interpretation of PCA Visualization
print("\n--- Interpretation of PCA Visualization ---")
print("The PCA visualization shows a reasonable separation between the clusters.")
print("- Cluster 1 (Lower Income, Low Spending) appears distinct from the other clusters, occupying a separate region.")
print("- Clusters 2 and 3 (Higher Income, High Spending) are somewhat overlapping but generally occupy a different area than Cluster 1, indicating their higher engagement.")
print("- Cluster 0 (Moderate) seems to be positioned somewhat between the lower and higher spending/income groups.")
print("The visualization generally aligns with the characteristics observed in the cluster profiles, with income and spending likely being major drivers of the separation along the principal components.")

# Add the 'Cluster' column back to the original DataFrame
df['Cluster'] = kmeans_final.labels_

display(df.head())

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

classifier = RandomForestClassifier(criterion = 'entropy')
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

final = pd.DataFrame()
final["Acctuate"] = y_test
final["Predicted"] = y_pred
final

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming 'df' is your DataFrame with the 'Cluster' column
# Select features (X) and target (y)
X = df.drop('Cluster', axis=1)
y = df['Cluster']

# Standardize the features (important for Logistic Regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)

# Train a Logistic Regression model
log_reg = LogisticRegression(random_state=0, multi_class='auto') # 'auto' selects the solver based on the data
log_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test)

# Evaluate the model
print("Logistic Regression Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming 'df' is your DataFrame with the 'Cluster' column
# Select features (X) and target (y)
X = df.drop('Cluster', axis=1)
y = df['Cluster']

# Standardize the features (important for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)

# Train an SVC model
svc_model = SVC(random_state=0)
svc_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svc_model.predict(X_test)

# Evaluate the model
print("Support Vector Classification (SVC) Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming 'df' is your DataFrame with the 'Cluster' column
# Select features (X) and target (y)
X = df.drop(['Total_Spent'], axis=1) # Exclude 'Total_Spent' and 'Cluster' from features
y = df['Total_Spent'] # Target variable is 'Total_Spent'

# Standardize the features (important for SVR)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)

# Train an SVR model
# You might need to tune the parameters (C, epsilon, kernel) for better performance
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svr_model.predict(X_test)

# Evaluate the model
print("Support Vector Regression (SVR) Evaluation for Total_Spent:")
print(f"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.2f}")
print(f"R-squared (R2): {r2_score(y_test, y_pred):.2f}")
print(f"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.2f}")

# Plot actual vs predicted values for SVR
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Add a diagonal line for reference
plt.xlabel('Actual Total_Spent')
plt.ylabel('Predicted Total_Spent')
plt.title('SVR: Actual vs Predicted Total_Spent')
plt.grid(True)
plt.show()



from sklearn.linear_model import LassoCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Assuming 'df' is your DataFrame with the 'Cluster' column
# Select features (X) and target (y)
# Exclude 'Total_Spent' and 'Cluster' from features
X = df.drop(['Total_Spent', 'Cluster'], axis=1)
y = df['Total_Spent'] # Target variable is 'Total_Spent'

# Standardize the features
scaler_lasso = StandardScaler()
X_scaled_lasso = scaler_lasso.fit_transform(X)

# Split the data into training and testing sets
X_train_lasso, X_test_lasso, y_train_lasso, y_test_lasso = train_test_split(X_scaled_lasso, y, test_size=0.25, random_state=0)

# Train a Lasso Regression model
# LassoCV finds the optimal alpha (regularization strength) using cross-validation
lasso_reg = LassoCV(cv=5, random_state=0, n_jobs=-1)
lasso_reg.fit(X_train_lasso, y_train_lasso)

# Make predictions on the test set
y_pred_lasso = lasso_reg.predict(X_test_lasso)

# Evaluate the model
print("Lasso Regression Evaluation for Total_Spent:")
print(f"Mean Squared Error (MSE): {mean_squared_error(y_test_lasso, y_pred_lasso):.2f}")
print(f"R-squared (R2): {r2_score(y_test_lasso, y_pred_lasso):.2f}")
print(f"Mean Absolute Error (MAE): {mean_absolute_error(y_test_lasso, y_pred_lasso):.2f}")

# Display the coefficients
print("\nLasso Coefficients:")
lasso_coefficients = pd.Series(lasso_reg.coef_, index=X.columns)
print(lasso_coefficients[lasso_coefficients != 0].sort_values(ascending=False)) # Show only non-zero coefficients

from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Assuming 'df' is your DataFrame with the 'Cluster' column
# Select features (X) and target (y)
X = df.drop('Cluster', axis=1)
y = df['Cluster']

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=0)

# Define the individual classifiers
clf1 = LogisticRegression(random_state=0, multi_class='auto')
clf2 = RandomForestClassifier(random_state=0)
clf3 = SVC(probability=True, random_state=0) # SVC needs probability=True for soft voting

# Create the Voting Classifier
# 'soft' voting uses predicted probabilities, 'hard' voting uses predicted class labels
eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')
eclf1 = eclf1.fit(X_train, y_train)
y_pred_ensemble = eclf1.predict(X_test)

# Evaluate the ensemble model
print("Voting Ensemble (Soft Voting) Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ensemble))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_ensemble))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_ensemble))

# You can also try 'hard' voting
eclf2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')
eclf2 = eclf2.fit(X_train, y_train)
y_pred_ensemble_hard = eclf2.predict(X_test)

print("\nVoting Ensemble (Hard Voting) Evaluation:")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_ensemble_hard))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_ensemble_hard))
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred_ensemble_hard))

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Define features (Total_Spent is the target, so it is REMOVED from the feature list)
regression_features = ['Age', 'Income', 'Education_Encoded', 'Total_Purchases', 'Family_Size', 'Recency', 'NumWebVisitsMonth']
X_reg = df[regression_features]
y_reg = df['Total_Spent']

# Split data
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42)

scaler_reg = StandardScaler()
X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
X_test_reg_scaled = scaler_reg.transform(X_test_reg)
# Train SVR for spending prediction
svr_model = SVR(kernel='rbf', C=100, gamma=0.1)
svr_model.fit(X_train_reg_scaled, y_train_reg)

# Make predictions
y_pred_svr = svr_model.predict(X_test_reg_scaled)

# Evaluate model
svr_r2 = r2_score(y_test_reg, y_pred_svr)
svr_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_svr))
svr_mae = mean_absolute_error(y_test_reg, y_pred_svr)

print(f"SVR Performance:")
print(f"R¬≤ Score: {svr_r2:.4f}")
print(f"RMSE: {svr_rmse:.2f}")
print(f"MAE: {svr_mae:.2f}")

# Plot actual vs predicted
plt.figure(figsize=(10, 8))
plt.scatter(y_test_reg, y_pred_svr, alpha=0.6)
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)
plt.xlabel('Actual Total Spent')
plt.ylabel('Predicted Total Spent')
plt.title(f'SVR: Actual vs Predicted Total Spending (R¬≤ = {svr_r2:.4f})')
plt.tight_layout()
plt.show()

x = df.drop('Cluster', axis=1)
y = df['Cluster']

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=42)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.pipeline import Pipeline

# Define the transformations for each column type
# 'Education_Encoded' is already ordinally encoded
# No specific transformer needed for Education_Encoded in this preprocessor
# if it's already in the desired numerical format.

# 'Marital_Status' was mapped to binary (Partner/Single)
# Assuming 'Partner' was mapped to 1 and 'Single' to 0, it's already numerical
# We can scale this binary numerical column

# 'Family_Size' is numerical, we can scale it


# Create the column transformer to apply different transformations to different columns
# 'Education_Encoded', 'Family_Size', and 'marital_status' are now numerical
# We will scale 'Family_Size' and 'marital_status' and pass the rest through,
# including 'Education_Encoded' as it's already numerical.
preprocessor = ColumnTransformer(
    transformers=[
        ('numerical_scaler', StandardScaler(), ['Income', 'Recency', 'MntWines', 'MntFruits',
                                               'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',
                                               'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',
                                               'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',
                                               'Age', 'Days_As_Customer', 'ChildrenCount', 'Family_Size',
                                               'marital_status', 'Total_Spent', 'Total_Purchases',
                                               'Total_Campaign_Accepted', 'Education_Encoded']) # Include all numerical and already encoded columns
    ],
    remainder='passthrough' # This should not be needed if all relevant columns are in the transformer list
)


# Display the preprocessor (optional)
print("Preprocessor created:")
print(preprocessor)

from sklearn.pipeline import make_pipeline
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

# Define the individual classifiers (using the same ones as before)
clf1 = LogisticRegression(random_state=0, multi_class='auto')
clf2 = RandomForestClassifier(random_state=0)
clf3 = SVC(probability=True, random_state=0) # SVC needs probability=True for soft voting

# Create the Voting Classifier
eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')

# Create the final pipeline
final_pipeline = make_pipeline(preprocessor, eclf1)

print("Final pipeline created:")
print(final_pipeline)

final_pipeline.fit(x_train, y_train)

final_pipeline.score(x_test, y_test)

import pickle


# Save the trained pipeline to a pickle file
filename = 'classifier.pkl'
with open(filename, 'wb') as file:
    pickle.dump(final_pipeline, file)

print("‚úÖ classifier.pkl file successfully created!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile customer_segmentation_app.py
# import streamlit as st
# import pandas as pd
# import pickle
# import os
# 
# # --- Load Model ---
# MODEL_PATH = "classifier.pkl"
# if not os.path.exists(MODEL_PATH):
#     st.error("‚ùå Model file not found! Upload classifier.pkl in this directory.")
#     st.stop()
# 
# with open(MODEL_PATH, "rb") as f:
#     model = pickle.load(f)
# 
# st.set_page_config(page_title="Customer Segmentation App", layout="centered")
# st.title("üß† Customer Segmentation Web App")
# 
# # --- Input Fields ---
# st.markdown("### Enter Customer Details Below:")
# income = st.number_input("Household Income", min_value=0, step=1000)
# kidhome = st.selectbox("Number of Kids in Household", [0, 1, 2])
# teenhome = st.selectbox("Number of Teens in Household", [0, 1, 2])
# age = st.slider("Age", 18, 85, 30)
# partner = st.selectbox("Living with Partner?", ["Yes", "No"])
# education = st.selectbox("Education Level", ["Undergraduate", "Graduate", "Postgraduate"])
# 
# # --- Prepare Input for Model ---
# input_data = pd.DataFrame([{
#     "Income": income,
#     "Kidhome": kidhome,
#     "Teenhome": teenhome,
#     "Age": age,
#     "Partner": partner,
#     "Education_Level": education
# }])
# 
# st.write("#### Model Input Preview:")
# st.dataframe(input_data)
# 
# # --- Predict Button ---
# if st.button("Segment Customer"):
#     try:
#         prediction = model.predict(input_data)[0]
#         st.success(f"üéØ Predicted Customer Segment: Cluster {prediction}")
#     except Exception as e:
#         st.error("‚ö†Ô∏è Prediction Failed ‚Äî Check Data Types or Columns")
#         st.exception(e)
#

from google.colab import files
uploaded = files.upload()   # select classifier.pkl
print(uploaded.keys())

!pip install -q streamlit
!npm install -g localtunnel

